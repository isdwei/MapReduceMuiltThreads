 3406
KeyProvider 2
timestamp. 1
allowed 2
log4j.category.SecurityLogger=${hadoop.security.logger} 1
without 1
LogLevel 2
setting 7
Otherwise 2
<name>hadoop.security.keystore.JavaKeyStoreProvider.password</name> 1
from. 1
log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN 1
mapreduce.cluster.acls.enabled 3
log4j.appender.kms-audit=org.apache.log4j.DailyRollingFileAppender 1
hadoop.security.log.maxbackupindex=20 1
logs.</description> 1
( 13
#rpc.class=org.apache.hadoop.metrics.file.FileContext 1
version 1
"jks". 4
Here 1
mapred.servers=localhost:8649 1
applies 2
HADOOP_JAVA_PLATFORM_OPTS="-XX:-UsePerfData 1
BlockManager 1
his/her 1
IS" 24
!= 3
"" 6
YARN_NODEMANAGER_HEAPSIZE=1000 1
</acl-submit-job> 1
how 1
<name>ssl.client.keystore.type</name> 1
log4j.appender.console.target=System.err 1
after 3
override 5
agreed 24
contain 2
-Dyarn.log.dir=$YARN_LOG_DIR" 1
'kms.log.dir' 1
ZNode 1
log4j.appender.MRAUDIT.layout=org.apache.log4j.PatternLayout 1
log4j.additivity.kms-audit=false 1
ContainerManagementProtocol 1
log4j.appender.MRAUDIT.MaxBackupIndex=${mapred.audit.log.maxbackupindex} 1
maximum 4
information 12
under 84
resolve 2
<name>ssl.client.keystore.keypassword</name> 1
language 24
Http 1
$0 1
%c{2} 2
log4j.additivity.org.apache.hadoop.mapred.AuditLogger=false 1
jobs. 2
priorities. 1
<queues> 1
Version 24
HADOOP_IDENT_STRING=$USER 1
HADOOP_SECURE_DN_LOG_DIR=${HADOOP_LOG_DIR}/${HADOOP_HDFS_USER} 1
export 41
seconds). 2
#log4j.additivity.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=false 1
<name>hadoop.kms.acl.GET_KEYS</name> 1
HADOOP_MAPRED_PID_DIR= 1
its 2
HTTP 4
hostnames 1
check 5
users 27
%HADOOP_NAMENODE_OPTS% 1
<name>yarn.scheduler.capacity.maximum-am-resource-percent</name> 1
log4j.logger.org.apache.hadoop.conf=ERROR 1
specified, 3
therefore 3
child 1
#mrappmaster.sink.file.filename=mrappmaster-metrics.out 1
some 2
Tomcat 3
protocol, 2
<description>Keystore 2
type="text/xsl" 6
log4j.appender.httpfs=org.apache.log4j.DailyRollingFileAppender 1
midnight 1
DN. 2
'none' 1
multiple 4
log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout 1
#log4j.logger.BlockStateChange=WARN 1
'${kms.home}/logs' 1
KMS_MAX_THREADS=1000 1
Backend 1
there 2
HADOOP_SECURITY_LOGGER=INFO,RFAS 1
#log4j.appender.datanoderequestlog.Filename=${hadoop.log.dir}/jetty-datanode-yyyy_mm_dd.log 1
(specified 1
</queue> 3
#nodemanager.sink.file_mapred.filename=nodemanager-mapred-metrics.out 1
tomcat 1
YARN_LOG_DIR="$HADOOP_YARN_HOME/logs" 1
Users 1
hadoop.tasklog.taskid=null 1
#log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG 1
#*.sink.ganglia.slope=jvm.metrics.gcCount=zero,jvm.metrics.memHeapUsedM=both 1
<name>ssl.server.truststore.location</name> 1
filenames 1
'*', 2
#mapreduce.hs.audit.logger=INFO,HSAUDIT 1
cookie 2
hot-reloaded 1
</properties> 2
client-to-datanode 1
get-current-key 1
printed 1
milliseconds. 4
messages 2
log4j.appender.MRAUDIT=org.apache.log4j.RollingFileAppender 1
${HTTPFS_HTTP_PORT} 1
HADOOP_SECURE_DN_PID_DIR=${HADOOP_PID_DIR} 1
HADOOP_NAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} 1
Application 2
other 1
clients 5
<description>ACL 21
<name>hadoop.kms.acl.GENERATE_EEK</name> 1
DominantResourceCalculator 1
JAVA_HOME 3
#nodemanager.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649 1
MRClientProtocol, 1
dfs.servers=localhost:8649 1
URI 1
hadoop.security.logger=INFO,NullAppender 1
file 62
[%X{hostname}][%X{user}:%X{doAs}] 2
administer 1
map 2
max 3
change 2
defining 1
#yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY 1
2.0 24
(Kerberos). 1
HttpFSServer 1
InterDatanodeProtocol, 1
return 1
log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize} 1
#namenode.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649 1
ugi.class=org.apache.hadoop.metrics.spi.NullContext 1
CPU 1
<value>random</value> 1
Counter 1
'zookeeper'. 1
':' 1
YARN_HEAPSIZE=1000 1
picked 3
<name>ssl.server.keystore.keypassword</name> 1
#log4j.appender.nodemanagerrequestlog.RetainDays=3 1
HADOOP_YARN_USER 1
new 2
level 6
<name>yarn.scheduler.capacity.root.queues</name> 1
-Djava.net.preferIPv4Stack=true" 1
retrieve 1
softlink 1
File 2
#log4j.appender.resourcemanagerrequestlog.RetainDays=3 1
NN 3
%5p 2
log4j.logger.com.amazonaws.http.AmazonHttpClient=ERROR 1
keys 1
jvm.servers=localhost:8649 1
<value>DEFAULT</value> 1
hadoop.log.maxbackupindex=20 1
best 2
ANY 24
#jvm.class=org.apache.hadoop.metrics.file.FileContext 1
HEAP 1
href="configuration.xsl"?> 6
ResourceCalculator 1
YARN_ROOT_LOGGER=%HADOOP_LOGLEVEL%,console 1
heap 2
queues. 3
KMS_HTTP_PORT=16000 1
rpc.servers=localhost:8649 1
service-level 2
any 5
SSL 9
HADOOP_CLIENT_OPTS=-Xmx512m 1
#log4j.appender.HSAUDIT=org.apache.log4j.DailyRollingFileAppender 1
<value>100</value> 2
<name>security.client.protocol.acl</name> 1
application 1
Zookeeper 3
#log4j.appender.namenoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender 1
log4j.appender.MRAUDIT.MaxFileSize=${mapred.audit.log.maxfilesize} 1
datanode 2
<name>security.admin.operations.protocol.acl</name> 1
CREATE 1
counts 1
#dfs.class=org.apache.hadoop.metrics.file.FileContext 1
natively 1
History 1
(default), 1
principal. 1
-Dhadoop.log.file=%YARN_LOGFILE% 1
log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender 1
options. 2
element 1
log4j.appender.httpfs.layout.ConversionPattern=%d{ISO8601} 1
-Dhdfs.audit.logger=%HDFS_AUDIT_LOGGER% 2
using 14
log4j.appender.httpfsaudit.layout=org.apache.log4j.PatternLayout 1
HADOOP_JHS_LOGGER=INFO,RFA 1
"$JAVA_HOME" 2
want 1
them 1
accept 2
secondary 1
<name>ssl.server.keystore.type</name> 1
HADOOP_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} 1
containers. 1
starting 3
directory) 1
access 1
killing 1
#nodemanager.sink.file_jvm.context=jvm 1
"x" 1
key="capacity" 1
<description>Optional. 8
window, 1
ASF 12
are 44
<value>default</value> 1
supports 1
</xsl:stylesheet> 1
#resourcemanager.sink.file.filename=resourcemanager-metrics.out 1
so 3
<value>kerberos</value> 1
modifying 1
HADOOP_CLASSPATH=$f 1
schedulers, 1
-Djava.library.path=$JAVA_LIBRARY_PATH" 1
single 1
log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file} 1
handled 1
</html> 1
#nodemanager.sink.file.filename=nodemanager-metrics.out 1
(former) 2
express 24
SPNEGO 1
scheduling 2
w/ 1
commands 3
/tmp 3
one: 2
log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log 1
#log4j.appender.jobhistoryrequestlog.RetainDays=3 1
query 1
deprecation 1
$HADOOP_PORTMAP_OPTS" 1
HADOOP_SECURE_DN_PID_DIR=%HADOOP_PID_DIR% 1
<name>yarn.scheduler.capacity.root.default.user-limit-factor</name> 1
hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log 1
site-specific 4
$HADOOP_CLIENT_OPTS" 1
configuration. 2
submit 2
sinks 1
#echo 1
prefix. 1
<?xml 12
log4j.rootLogger=${hadoop.root.logger}, 1
<name>fs.defaultFS</name> 1
match="configuration"> 1
ApplicationHistoryProtocol, 1
<name>default.key.acl.GENERATE_EEK</name> 1
HTTPFS_LOG=${HTTPFS_HOME}/logs 1
NOTE: 2
true. 3
window 1
JAVA_HOME=/home/y/libexec/jdk1.6.0/ 2
log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} 1
-Djava.library.path=%JAVA_LIBRARY_PATH% 1
e.g. 18
<name>ssl.client.truststore.location</name> 1
YARN_HEAPSIZE 1
logger 2
Security 2
method="html"/> 1
commas. 1
permissions 24
%c: 5
DatanodeProtocol, 1
case 1
generation 1
generateEncryptedKey 1
writing, 24
Log 3
ports. 2
priority 1
<value>kms/#HOSTNAME#</value> 1
log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN 1
WITHOUT 24
"Error: 1
NOTICE 12
getCurrentKey 2
log 14
signature 2
RefreshAuthorizationPolicyProtocol, 1
secret 3
Yarn 1
specified 13
"$YARN_HEAPSIZE" 1
version="1.0" 5
KIND, 24
<name>hadoop.kms.acl.ROLLOVER</name> 1
<value>10000</value> 4
border="1"> 1
Pattern 4
include 1
YARN_RESOURCEMANAGER_HEAPSIZE=1000 1
*.sink.ganglia.supportsparse=true 1
log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex} 1
Controller 1
YARN 5
#namenode.sink.file.filename=namenode-metrics.out 1
user? 1
*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30 1
#log4j.appender.HSAUDIT.layout.ConversionPattern=%d{ISO8601} 1
Authentication 2
submission 1
runs 3
%X{op} 2
YARN_CONF_DIR=%HADOOP_YARN_HOME%\conf 1
ugi.period=10 1
overridden 4
JAVA_HEAP_MAX=-Xmx%YARN_HEAPSIZE%m 1
KMS_LOG=${KMS_HOME}/logs 1
Maximum 2
protocol 6
</xsl:template> 1
<name>q2</name> 1
privileges. 1
decryptEncryptedKey 1
only 9
HAService 1
'*' 1
-Xmx1000m, 3
JAVA_LIBRARY_PATH=${HOME}/lib/native 1
<value>1</value> 1
runtime 3
history 1
CAN 1
Can 1
#resourcemanager.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649 1
HADOOP_CLIENT_OPTS="-Xmx512m 1
send 1
here 2
credentials 2
line 1
DistributedFileSystem. 1
'${httpfs.home}/logs' 1
<name>security.refresh.user.mappings.protocol.acl</name> 1
eg. 1
log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} 1
can 14
numerical 3
security 1
operations 8
"AS 24
CONDITIONS 24
<tr> 2
get-key-metadata 1
<body> 1
<td>value</td> 1
authentication 7
<name>hadoop.kms.authentication.kerberos.name.rules</name> 1
amount 2
log4j.appender.kms-audit.layout.ConversionPattern=%d{ISO8601} 1
-Dyarn.home.dir=%HADOOP_YARN_HOME% 1
groups 2
<configuration> 11
hadoop.tasklog.iscleanup=false 1
#*.sink.ganglia.tagsForPrefix.jvm=ProcesName 1
log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd 1
Extra 4
variable 4
HADOOP_PORTMAP_OPTS="-Xmx512m 1
log4j.logger.com.sun.jersey.server.wadl.generators.WadlGeneratorJAXBGrammarGenerator=OFF 1
uses 2
<name>security.mrhs.client.protocol.acl</name> 1
file, 2
