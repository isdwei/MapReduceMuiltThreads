Xmx 3
tasktracker. 1
Server 2
IFS= 1
rpc.class=org.apache.hadoop.metrics.ganglia.GangliaContext 1
flushed 1
HADOOP_JOB_HISTORYSERVER_HEAPSIZE=1000 2
users,wheel". 18
Define 2
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd 1
YARN_OPTS 7
HADOOP_SECURE_DN_LOG_DIR=%HADOOP_LOG_DIR%\%HADOOP_HDFS_USER% 1
you 28
# 384
<name>hadoop.kms.authentication.type</name> 1
' 3
log4j.appender.RFA.layout=org.apache.log4j.PatternLayout 1
+ 2
log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger} 1
min.user.id=1000#Prevent 1
distcp 2
See 46
log4j.appender.httpfsaudit=org.apache.log4j.DailyRollingFileAppender 1
log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender 1
log4j.appender.TLA.totalLogFileSize=${hadoop.tasklog.totalLogFileSize} 1
at 31
metrics 3
Where 5
TaskUmbilicalProtocol, 1
<description>Default 1
HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$f 1
dominant-resource 1
be 48
(10 2
&quot;kerberos&quot;. 1
NodeManager. 1
#jvm.class=org.apache.hadoop.metrics.spi.NullContext 1
log4j.appender.kms.File=${kms.log.dir}/kms.log 1
by 90
HADOOP_SECONDARYNAMENODE_OPTS=-Dhadoop.security.logger=%HADOOP_SECURITY_LOGGER% 1
[ 8
it. 1
loops 1
Rules 1
Audit 1
By 1
returned 2
communicate 10
	<name>hadoop.tmp.dir</name> 1
HADOOP_DATANODE_OPTS=-Dhadoop.security.logger=ERROR,RFAS 1
</configuration> 11
do 6
*.period=10 1
<value>${user.home}/kms.keytab</value> 1
Modifications 1
correctly 3
yarn.server.resourcemanager.appsummary.log.file 1
FileSystem 1
view, 1
syntax: 1
others 2
'(i.e. 2
Logging 2
Appender 7
%HADOOP_DATANODE_OPTS% 1
allowed.</description> 18
NamenodeProtocol, 1
count 1
appender 3
regarding 12
format: 2
<name>ssl.client.truststore.type</name> 1
fi 8
additional 12
TaskLog 1
1.0. 1
"jvm" 3
ownership. 12
specification. 1
client 2
end 1
applications. 1
hadoop.root.logger=INFO,console 1
owner 1
getKeyVersion 1
Memory, 1
'. 1
false 1
enabled, 1
%p 11
Admin 2
message 1
schedule 1
environment 6
false. 1
material 3
Command 2
YARN_OPTS="$YARN_OPTS 10
KMS_ADMIN_PORT=`expr 1
<value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value> 1
syntax 1
jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext 1
#log4j.logger.http.requests.resourcemanager=INFO,resourcemanagerrequestlog 1
assign 1
period, 1
SASL 2
copyright 12
mapred.audit.logger=INFO,NullAppender 1
fsck, 2
get-keys 1
log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender 1
Sends 1
HS 1
<description>Must 2
log4j.appender.httpfs.DatePattern='.'yyyy-MM-dd 1
"$YARN_LOGFILE" 1
log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger} 1
"alice,bob 18
context 18
#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} 1
-Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} 2
Settings 2
<name>ssl.server.truststore.type</name> 1
similar 3
#log4j.appender.datanoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender 1
distributed 62
<properties> 2
it 9
log4j.appender.RMSUMMARY.File=${hadoop.log.dir}/${yarn.server.resourcemanager.appsummary.log.file} 1
<acl-submit-job> 1
system 3
YARN_NODEMANAGER_OPTS. 1
Jets3t 1
name. 1
<name>ssl.server.keystore.password</name> 1
HADOOP_MAPRED_ROOT_LOGGER=INFO,RFA 1
environment. 2
server 3
Required. 1
#Security 1
It 2
library 1
native 1
#rpc.period=10 1
within 4
missed 1
1000. 5
Use 1
overrides 4
off 4
(latter) 2
ResourceManagerAdministrationProtocol, 1
stored 2
irrespective 1
#log4j.logger.http.requests.namenode=INFO,namenoderequestlog 1
Null 1
<?xml-stylesheet 6
use 30
pid 3
<name>security.job.task.protocol.acl</name> 1
that 20
%-5p 3
log4j.appender.TLA.layout=org.apache.log4j.PatternLayout 1
limit 1
yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger} 1
log4j.appender.kms.layout=org.apache.log4j.PatternLayout 1
dfs.class=org.apache.hadoop.metrics.ganglia.GangliaContext 1
logs 2
key. 1
xmlns:xsl="http://www.w3.org/1999/XSL/Transform" 1
-f` 1
applicable 24
null 5
<name>ssl.client.keystore.password</name> 1
stopped, 1
hdfs.audit.logger=INFO,NullAppender 1
YARN_POLICYFILE=hadoop-policy.xml 1
<td><a 1
running 3
and 93
of 139
OF 24
YARN_LOG_DIR 1
JavaKeyStoreProvider, 1
Expiry 2
on 30
scheduler. 1
</table> 1
console 1
or 71
OR 24
MANAGEMENT 1
AdminOperationsProtocol. 1
namenode-metrics.out 1
<value>simple</value> 1
Logs 1
string, 1
'HTTP/' 1
interval, 2
properties 7
On 2
metadata 1
Automatically 2
#dfs.period=10 1
yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log 1
log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender 1
#*.sink.ganglia.tagsForPrefix.mapred= 1
hostname 1
password 3
Metrics. 1
-Dyarn.log.file=%YARN_LOGFILE% 1
Foundation 12
daemons. 3
LoggerName 2
$HADOOP_MAPRED_HOME/logs 1
datanodes. 1
sampling 2
portable 1
		<name>yarn.resourcemanager.hostname</name> 1
YARN_CONF_DIR="${YARN_CONF_DIR:-$HADOOP_YARN_HOME/conf}" 1
NodeManager 4
ResourceManager. 1
filename 1
YARN_TIMELINESERVER_HEAPSIZE=1000 1
dfs, 2
RefreshUserMappingsProtocol. 1
communciate 2
<name>hadoop.kms.key.provider.uri</name> 1
		<name>yarn.nodemanager.aux-services</name> 1
concurrent 1
cookies 1
document. 1
1` 2
queue, 1
audit 4
encoding="UTF-8"?> 5
where 4
<name>security.containermanagement.protocol.acl</name> 1
distcp. 2
key 10
variables 4
HADOOP_NAMENODE_OPTS=-Dhadoop.security.logger=%HADOOP_SECURITY_LOGGER% 1
KMS_SSL_KEYSTORE_PASS=password 1
HADOOP_MAPRED_LOG_DIR="" 1
refresh 2
datanodes 2
<name>security.zkfc.protocol.acl</name> 1
such 1
absolute 1
banned.users=#comma 1
to 165
HADOOP_NFS3_OPTS="$HADOOP_NFS3_OPTS" 1
elements. 2
package-info.java 1
separate 2
HTTPFS_SSL_KEYSTORE_FILE=${HOME}/.keystore 1
log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender 1
run 10
source 2
secure 7
Name 1
Kerberos 6
*.sink.ganglia.period=10 1
<name>hadoop.kms.authentication.signer.secret.provider.zookeeper.kerberos.principal</name> 1
segment 1
written 2
up 2
To 1
"$YARN_POLICYFILE" 1
maps 1
RUNNING 1
might 1
#jvm.period=10 1
get-key-version 1
log4j.appender.kms-audit.DatePattern='.'yyyy-MM-dd 1
log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file} 1
privileged 2
<name>default.key.acl.READ</name> 1
These 3
<value>600000</value> 1
name 3
supportsparse 1
support 2
<name>security.inter.datanode.protocol.acl</name> 1
full 1
version="1.0"?> 7
pending 1
changes 1
admin 2
System 4
not 51
HADOOP_CLASSPATH 1
backing 3
-Dhadoop.log.dir=$YARN_LOG_DIR" 1
and/or 3
non-privileged 2
allowed.system.users=##comma 1
normal 1
rack 1
Request 1
#log4j.logger.http.requests.nodemanager=INFO,nodemanagerrequestlog 1
		<value>mapreduce_shuffle</value> 1
Put 4
generic 1
warnings. 1
YARN_RESOURCEMANAGER_OPTS. 1
rollover-key 1
applications 4
Protocols 1
software 24
KMS_SSL_KEYSTORE_FILE=${HOME}/.keystore 1
'string' 1
HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/etc/hadoop"} 1
log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} 1
required 29
seconds 1
enable 2
HADOOP_CLASSPATH=%HADOOP_HOME%\contrib\capacity-scheduler\*.jar 1
-Dyarn.id.str=$YARN_IDENT_STRING" 1
#log4j.appender.datanoderequestlog.RetainDays=3 1
Threshold 1
#log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} 1
exist 1
#export 15
<value>true</value> 1
logging 4
log4j.threshold=ALL 1
running. 1
"dfs" 3
Site 1
ms) 1
NOT** 1
<name>yarn.scheduler.capacity.root.default.capacity</name> 1
ResourceLocalizer 2
flags 3
hadoop.tasklog.logsRetainHours=12 1
Irrespective 2
-Dyarn.id.str=%YARN_IDENT_STRING% 1
log4j.appender.httpfs.Append=true 1
<xsl:template 1
log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex} 1
hadoop.log.dir=. 1
daemons 2
place 1
stored. 7
kms-audit 1
user. 2
<html> 1
#jobhistoryserver.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649 1
LogMessage 2
cannot 1
levels 2
"mapred" 3
log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize} 1
ugi.class=org.apache.hadoop.metrics.ganglia.GangliaContext31 1
HTTPFS_ADMIN_PORT=`expr 1
hadoop.mapreduce.jobsummary.logger=INFO,JSA 1
should 6
operation. 3
Licensed 24
checked 1
group 36
localhost 1
log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} 1
like 3
format 5
ResourceManager 8
JNs 1
use. 4
log4j.appender.kms-audit.Append=true 1
transfer 4
<description>Truststore 4
files 8
Software 12
<xsl:stylesheet 1
### 4
User 2
(fs, 2
part 2
getMetadata, 1
$USER 2
License, 24
contributor 12
accompanying 9
mapred.class=org.apache.hadoop.metrics.ganglia.GangliaContext 1
#log4j.logger.http.requests.jobhistory=INFO,jobhistoryrequestlog 1
log4j.appender.kms-audit.File=${kms.log.dir}/kms-audit.log 1
HTTPFS_SSL_KEYSTORE_PASS=password 1
#log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG 1
also 1
"rpc" 3
Specifies 2
governing 24
mapping]* 1
compliance 24
XML 1
Duplicate 1
HADOOP_PID_DIR=%HADOOP_PID_DIR% 1
$HADOOP_HOME/contrib/capacity-scheduler/*.jar; 1
user 48
You 33
