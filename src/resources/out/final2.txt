blank. 18
appender) 1
cluster 5
parent 1
handler 1
#log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG 1
Configuration 17
&quot;simple&quot; 1
Node 1
$HADOOP_SECONDARYNAMENODE_OPTS" 1
remote 2
commands. 2
via 3
java 3
sizes 1
cluster, 1
KeyProvider. 2
Embedded 2
SDK 1
指定YARN的ResourceManager的地址 1
implied. 24
cache 2
AppSummaryLogging 1
edit 1
& 2
-Dhadoop.log.file=$YARN_LOGFILE" 1
active 1
Apache 37
<td>name</td> 1
as 15
Defaults 1
-Dyarn.root.logger=%YARN_ROOT_LOGGER% 1
#jobhistoryserver.sink.file.filename=jobhistoryserver-metrics.out 1
%USERNAME% 1
Note 2
#nodemanager.sink.file_jvm.filename=nodemanager-jvm-metrics.out 1
: 1
<name>ssl.client.truststore.password</name> 1
i.e. 2
rpc.class=org.apache.hadoop.metrics.spi.NullContext 1
hadoop.tasklog.noKeepSplits=4 1
JAVA_HOME. 2
Set 6
log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender 1
@rem 72
5gb. 1
ports 2
log4j.appender.httpfsaudit.Append=true 1
[u|g]:[name]:[queue_name][,next 1
example 1
q1. 1
javadoc 1
same 1
<name>hadoop.kms.cache.timeout.ms</name> 1
JAVA_HEAP_MAX 3
HSClientProtocol, 1
log4j.appender.kms=org.apache.log4j.DailyRollingFileAppender 1
#log4j.logger.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=${mapreduce.hs.audit.logger} 1
%c{2}: 6
connect 2
policy 3
<value>file:/Hadoop/temp/dfs/namenode</value> 1
hadoop-env.sh 1
Java 10
set 60
log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} 1
f 1
consulting 1
hadoop.mapreduce.jobsummary.log.file 1
one 15
jobs 10
representing 3
ACL 36
HADOOP_HEAPSIZE= 2
(resource 2
		<value>localhost</value> 1
sample 1
licenses 12
log4j.appender.JSA.layout=org.apache.log4j.PatternLayout 1
<acl-administer-jobs> 1
<name>yarn.scheduler.capacity.root.default.acl_submit_applications</name> 1
operations. 9
Event 1
<value>jceks://file@/${user.home}/kms.keystore</value> 1
<name>security.refresh.policy.protocol.acl</name> 1
#A 1
supported 1
privileges 1
controls 1
'queues' 1
JAVA_LIBRARY_PATH 1
#datanode.sink.file.filename=datanode-metrics.out 1
sometimes 1
keytab 2
acl 1
%YARN_HEAPSIZE% 1
指定HDFS中NameNode的地址 1
<name>yarn.scheduler.capacity.maximum-applications</name> 1
%HADOOP_CLIENT_OPTS% 1
act 1
#log4j.appender.jobhistoryrequestlog=org.apache.hadoop.http.HttpRequestLogAppender 1
<name>hadoop.kms.authentication.signer.secret.provider.zookeeper.auth.type</name> 1
log4j.rootLogger=ALL, 1
#log4j.appender.jobhistoryrequestlog.Filename=${hadoop.log.dir}/jetty-jobhistory-yyyy_mm_dd.log 1
Cached 1
This 10
details 1
files) 1
httpfsaudit 1
has 1
Unless 24
--> 39
cache, 2
jobs, 1
log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd 1
select="name"/></a></td> 1
last 1
<value>hdfs://localhost:9000</value> 1
need 1
this 78
<name>hadoop.kms.current.key.cache.timeout.ms</name> 1
type, 1
please 1
list 44
instances, 1
log4j.logger.org.apache.hadoop.fs.http.server=INFO, 1
<name>default.key.acl.DECRYPT_EEK</name> 1
HTTPFS_HTTP_HOSTNAME=`hostname 1
specified. 6
HAAdmin 1
<xsl:for-each 1
parameters 4
'httpfs.log.dir' 1
Cache 1
<name>hadoop.kms.acl.DECRYPT_EEK</name> 1
mapred.period=10 1
Options 1
hadoop.log.dir 1
YARN_RESOURCEMANAGER_OPTS= 1
"x$JAVA_LIBRARY_PATH" 1
mapreduce.cluster.administrators 2
dfsadmin 1
aggregation 1
<value>0.1</value> 1
configuration, 2
states 1
-Dyarn.home.dir=$YARN_COMMON_HOME" 1
protocol. 2
hadoop.mapreduce.jobsummary.log.maxfilesize=256MB 1
HDFS_AUDIT_LOGGER 1
jvm.period=10 1
root 2
IFS 1
log4j.appender.httpfsaudit.File=${httpfs.log.dir}/httpfs-audit.log 1
names. 19
"*" 18
mradmin 1
summary 5
log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} 1
-Dyarn.log.dir=%YARN_LOG_DIR% 1
other. 6
<name>security.applicationclient.protocol.acl</name> 1
appropriately 1
log4j.appender.RFAS=org.apache.log4j.RollingFileAppender 1
version="1.0"> 1
<name>security.resourcelocalizer.protocol.acl</name> 1
start 3
<name>hadoop.kms.authentication.signer.secret.provider</name> 1
affects 2
with 59
#log4j.appender.namenoderequestlog.RetainDays=3 1
modify 1
</property> 87
queues 9
<xsl:output 1
logger. 1
Zookeeper. 2
queue). 1
(APR) 1
$JAVA_HOME" 1
#log4j.appender.nodemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-nodemanager-yyyy_mm_dd.log 1
log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout 1
<name>ssl.server.truststore.password</name> 1
YARN_NODEMANAGER_OPTS= 1
yarn.nodemanager.linux-container-executor.group 1
ugi.servers=localhost:8649 1
log4j.appender.MRAUDIT.File=${hadoop.log.dir}/mapred-audit.log 1
WARRANTIES 24
#mapred.class=org.apache.hadoop.metrics.file.FileContext 1
type 1
		<name>mapreduce.framework.name</name> 1
when 9
set." 1
<name>hadoop.kms.acl.DELETE</name> 1
log4j.appender.JSA=org.apache.log4j.RollingFileAppender 1
log4j.logger.org.apache.hadoop.mapred.AuditLogger=${mapred.audit.logger} 1
options 11
keystore 9
connection 1
ACL, 2
tasks 2
attempts 1
rootlogger 1
reduce 2
#nodemanager.sink.file_jvm.class=org.apache.hadoop.metrics2.sink.FileSink 1
mapping 1
mapred.audit.log.maxfilesize=256MB 1
<name>default</name> 1
EventCounter 1
#log4j.appender.resourcemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender 1
is 120
ordinary 1
<name>security.namenode.protocol.acl</name> 1
log4j.logger.org.apache.hadoop=INFO 1
optional. 2
exit 1
40. 1
element. 1
Failover 1
etc. 3
3.1 3
GET 2
log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter 1
JAVA_HEAP_MAX="-Xmx""$YARN_HEAPSIZE""m" 1
super-users 1
status 2
log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize} 1
template 1
<state>running</state> 1
submitting 1
Advanced 1
HTTPFS_HTTP_PORT=14000 1
log4j.logger.kms-audit=INFO, 1
<name>hadoop.kms.audit.aggregation.window.ms</name> 1
capacity 1
Complementary 1
log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file} 1
feature 1
two. 3
<name>hadoop.kms.authentication.kerberos.keytab</name> 1
cached 1
filename) 1
attack. 2
sending 1
#nodemanager.sink.file_mapred.class=org.apache.hadoop.metrics2.sink.FileSink 1
nesting 1
<value>/hadoop-kms/hadoop-auth-signature-secret</value> 1
#The 1
means 18
ResourceTrackerProtocol, 1
CryptoExtension 2
(root 1
*.sink.file.class=org.apache.hadoop.metrics2.sink.FileSink 1
<value>30000</value> 1
<name>hadoop.kms.cache.enable</name> 1
hadoop.security.log.file=SecurityAuth-${user.name}.audit 1
Debugging 2
HADOOP_OPTS 4
#nodemanager.sink.file_mapred.context=mapred 1
${KMS_HTTP_PORT} 1
leaf 2
#log4j.appender.namenoderequestlog.Filename=${hadoop.log.dir}/jetty-namenode-yyyy_mm_dd.log 1
When 3
JAVA=$JAVA_HOME/bin/java 1
指定Hadoop运行时产生文件的存储目录 1
<name>yarn.scheduler.capacity.root.default.maximum-capacity</name> 1
value="30"/> 1
log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger} 1
Tag 1
#log4j.appender.nodemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender 1
]; 8
example, 5
select="property"> 1
%HADOOP_SECONDARYNAMENODE_OPTS% 1
YARN_LOGFILE=yarn.log 1
YARN_LOGFILE='yarn.log' 1
data 7
create-key 1
log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex} 1
jsvc 2
$HADOOP_NAMENODE_OPTS" 1
#ugi.class=org.apache.hadoop.metrics.file.FileContext 1
org.apache.hadoop.metrics2 1
<name>hadoop.kms.authentication.kerberos.principal</name> 1
rpc.period=10 1
#Default 1
log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex} 1
space 1
YARN_TIMELINESERVER_OPTS. 1
stand-by 1
%HADOOP_HOME%\contrib\capacity-scheduler 1
split 1
updating 1
BASIS, 24
dfs.period=10 1
percentage 1
from 7
tag 1
potential 2
administrators 3
Hadoop-specific 2
configured 7
hdfs.audit.log.maxfilesize=256MB 1
law 24
ApplicationMasterProtocol, 1
Requires 1
log4j.appender.kms.layout.ConversionPattern=%d{ISO8601} 1
Uncomment 2
Resource 1
done 1
log4j.appender.RMSUMMARY.MaxFileSize=256MB 1
<name>yarn.scheduler.capacity.root.default.acl_administer_queue</name> 1
Currently, 1
READ 1
<name>yarn.scheduler.capacity.root.default.state</name> 1
log4j.appender.httpfsaudit.DatePattern='.'yyyy-MM-dd 1
HTTPFS_SSL_ENABLED=false 1
Indicates 2
select="value"/></td> 1
who 6
'zookeeper' 2
yarn.nodemanager.linux-container-executor.group=#configured 1
YARN_ROOT_LOGGER 1
configuration 9
instances 1
values 4
ApplicationMaster 1
HADOOP_YARN_USER=${HADOOP_YARN_USER:-yarn} 1
scale 3
-Dyarn.root.logger=${YARN_ROOT_LOGGER:-INFO,console}" 1
<name>default.key.acl.MANAGEMENT</name> 1
<description> 41
principal 4
cap 1
<name>yarn.scheduler.capacity.node-locality-delay</name> 1
	<!-- 1
here. 4
log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize} 1
<td><xsl:value-of 2
#log4j.appender.resourcemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-resourcemanager-yyyy_mm_dd.log 1
hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger} 1
allows 2
<table 1
dfs.class=org.apache.hadoop.metrics.spi.NullContext 1
restore 1
will 23
implementation 5
queues, 1
<name>security.qjournal.service.protocol.acl</name> 1
log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout 1
specific 33
capacity-scheduler. 2
Date 2
"". 4
<name>security.client.datanode.protocol.acl</name> 1
-Djava.net.preferIPv4Stack=true 1
0.0 1
<value>false</value> 1
mapred.class=org.apache.hadoop.metrics.spi.NullContext 1
namenode.</description> 1
(%F:%M(%L)) 2
specifiying 1
Jsvc 3
Summary 3
<value>RUNNING</value> 1
QJournalProtocol, 1
file. 14
0. 1
